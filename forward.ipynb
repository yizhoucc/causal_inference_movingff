{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from firefly_task import Model as Task\n",
    "from stable_baselines3 import PPO\n",
    "from ult import *\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(f'{os.getcwd()}')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'movingff_test0908'\n",
    "\n",
    "epoch_size = 33333\n",
    "n_epoch = 33\n",
    "n_seed = 1\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# for plotting\n",
    "num_eval_episode = 2222  # for eval plot during training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, yctore/seed0_modelmovingff_test0908_ep0 saved\n",
      "avg reward per eval trials: 0.58\n",
      "epoch1, yctore/seed0_modelmovingff_test0908_ep1 saved\n",
      "avg reward per eval trials: 0.64\n",
      "epoch2, yctore/seed0_modelmovingff_test0908_ep2 saved\n",
      "avg reward per eval trials: 0.60\n",
      "epoch3, yctore/seed0_modelmovingff_test0908_ep3 saved\n",
      "avg reward per eval trials: 0.58\n",
      "epoch4, yctore/seed0_modelmovingff_test0908_ep4 saved\n",
      "avg reward per eval trials: 0.57\n",
      "epoch5, yctore/seed0_modelmovingff_test0908_ep5 saved\n",
      "avg reward per eval trials: 0.64\n",
      "epoch6, yctore/seed0_modelmovingff_test0908_ep6 saved\n",
      "avg reward per eval trials: 0.57\n",
      "epoch7, yctore/seed0_modelmovingff_test0908_ep7 saved\n",
      "avg reward per eval trials: 0.67\n",
      "epoch8, yctore/seed0_modelmovingff_test0908_ep8 saved\n",
      "avg reward per eval trials: 0.69\n",
      "epoch9, yctore/seed0_modelmovingff_test0908_ep9 saved\n",
      "avg reward per eval trials: 0.66\n",
      "epoch10, yctore/seed0_modelmovingff_test0908_ep10 saved\n",
      "avg reward per eval trials: 0.64\n",
      "epoch11, yctore/seed0_modelmovingff_test0908_ep11 saved\n",
      "avg reward per eval trials: 0.66\n",
      "epoch12, yctore/seed0_modelmovingff_test0908_ep12 saved\n",
      "avg reward per eval trials: 0.63\n",
      "epoch13, yctore/seed0_modelmovingff_test0908_ep13 saved\n",
      "avg reward per eval trials: 0.65\n",
      "epoch14, yctore/seed0_modelmovingff_test0908_ep14 saved\n",
      "avg reward per eval trials: 0.66\n",
      "epoch15, yctore/seed0_modelmovingff_test0908_ep15 saved\n",
      "avg reward per eval trials: 0.61\n",
      "epoch16, yctore/seed0_modelmovingff_test0908_ep16 saved\n",
      "avg reward per eval trials: 0.66\n",
      "epoch17, yctore/seed0_modelmovingff_test0908_ep17 saved\n",
      "avg reward per eval trials: 0.65\n",
      "epoch18, yctore/seed0_modelmovingff_test0908_ep18 saved\n",
      "avg reward per eval trials: 0.72\n",
      "epoch19, yctore/seed0_modelmovingff_test0908_ep19 saved\n",
      "avg reward per eval trials: 0.64\n",
      "epoch20, yctore/seed0_modelmovingff_test0908_ep20 saved\n",
      "avg reward per eval trials: 0.66\n",
      "epoch21, yctore/seed0_modelmovingff_test0908_ep21 saved\n",
      "avg reward per eval trials: 0.63\n",
      "epoch22, yctore/seed0_modelmovingff_test0908_ep22 saved\n",
      "avg reward per eval trials: 0.64\n",
      "epoch23, yctore/seed0_modelmovingff_test0908_ep23 saved\n",
      "avg reward per eval trials: 0.61\n",
      "epoch24, yctore/seed0_modelmovingff_test0908_ep24 saved\n",
      "avg reward per eval trials: 0.66\n",
      "epoch25, yctore/seed0_modelmovingff_test0908_ep25 saved\n",
      "avg reward per eval trials: 0.67\n",
      "epoch26, yctore/seed0_modelmovingff_test0908_ep26 saved\n",
      "avg reward per eval trials: 0.73\n",
      "epoch27, yctore/seed0_modelmovingff_test0908_ep27 saved\n",
      "avg reward per eval trials: 0.67\n",
      "epoch28, yctore/seed0_modelmovingff_test0908_ep28 saved\n",
      "avg reward per eval trials: 0.63\n",
      "epoch29, yctore/seed0_modelmovingff_test0908_ep29 saved\n",
      "avg reward per eval trials: 0.70\n",
      "epoch30, yctore/seed0_modelmovingff_test0908_ep30 saved\n",
      "avg reward per eval trials: 0.65\n",
      "epoch31, yctore/seed0_modelmovingff_test0908_ep31 saved\n",
      "avg reward per eval trials: 0.63\n",
      "epoch32, yctore/seed0_modelmovingff_test0908_ep32 saved\n",
      "avg reward per eval trials: 0.63\n",
      "epoch33, yctore/seed0_modelmovingff_test0908_ep33 saved\n",
      "avg reward per eval trials: 0.70\n",
      "epoch34, yctore/seed0_modelmovingff_test0908_ep34 saved\n",
      "avg reward per eval trials: 0.57\n",
      "epoch35, yctore/seed0_modelmovingff_test0908_ep35 saved\n",
      "avg reward per eval trials: 0.71\n",
      "epoch36, yctore/seed0_modelmovingff_test0908_ep36 saved\n",
      "avg reward per eval trials: 0.68\n",
      "epoch37, yctore/seed0_modelmovingff_test0908_ep37 saved\n",
      "avg reward per eval trials: 0.67\n",
      "epoch38, yctore/seed0_modelmovingff_test0908_ep38 saved\n",
      "avg reward per eval trials: 0.63\n",
      "epoch39, yctore/seed0_modelmovingff_test0908_ep39 saved\n",
      "avg reward per eval trials: 0.69\n",
      "epoch40, yctore/seed0_modelmovingff_test0908_ep40 saved\n",
      "avg reward per eval trials: 0.62\n",
      "epoch41, yctore/seed0_modelmovingff_test0908_ep41 saved\n",
      "avg reward per eval trials: 0.61\n",
      "epoch42, yctore/seed0_modelmovingff_test0908_ep42 saved\n",
      "avg reward per eval trials: 0.66\n",
      "epoch43, yctore/seed0_modelmovingff_test0908_ep43 saved\n",
      "avg reward per eval trials: 0.64\n",
      "epoch44, yctore/seed0_modelmovingff_test0908_ep44 saved\n",
      "avg reward per eval trials: 0.66\n",
      "epoch45, yctore/seed0_modelmovingff_test0908_ep45 saved\n",
      "avg reward per eval trials: 0.67\n",
      "epoch46, yctore/seed0_modelmovingff_test0908_ep46 saved\n",
      "avg reward per eval trials: 0.59\n",
      "epoch47, yctore/seed0_modelmovingff_test0908_ep47 saved\n",
      "avg reward per eval trials: 0.64\n",
      "epoch48, yctore/seed0_modelmovingff_test0908_ep48 saved\n",
      "avg reward per eval trials: 0.62\n",
      "epoch49, yctore/seed0_modelmovingff_test0908_ep49 saved\n",
      "avg reward per eval trials: 0.62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     previous_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ep\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miepoch\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m     model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mycstore/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, env\u001b[38;5;241m=\u001b[39menv)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mycstore/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthismodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, yctore/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthismodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:70\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m---> 70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/shimmy/openai_gym_compatibility.py:235\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     warn(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym v21 environment do not accept options as a reset parameter, options=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 235\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/moving ff/firefly_task.py:91\u001b[0m, in \u001b[0;36mModel.reset\u001b[0;34m(self, theta, task)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# end of obs phase, is start for action phase. f, v, w\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m z_0, Sigma_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# f, v\u001b[39;00m\n\u001b[1;32m     92\u001b[0m f_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_t\n\u001b[1;32m     93\u001b[0m v_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/moving ff/firefly_task.py:159\u001b[0m, in \u001b[0;36mModel.obs_phase\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m     Sigma \u001b[38;5;241m=\u001b[39m Sigma[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m], :][:, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_selfmotion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_objmotion:\n\u001b[0;32m--> 159\u001b[0m     Sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_Sigma_stat_SM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     Sigma \u001b[38;5;241m=\u001b[39m Sigma[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m], :][:, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_selfmotion \u001b[38;5;129;01mand\u001b[39;00m is_objmotion:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/moving ff/firefly_task.py:623\u001b[0m, in \u001b[0;36mModel.get_Sigma_stat_SM\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    621\u001b[0m xi_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhxi_v)\n\u001b[1;32m    622\u001b[0m xi_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhxi_w)\n\u001b[0;32m--> 623\u001b[0m xi_f_ \u001b[38;5;241m=\u001b[39m xi_f\u001b[38;5;241m*\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_obs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39msig_f)\n\u001b[1;32m    624\u001b[0m xi_v_ \u001b[38;5;241m=\u001b[39m xi_v\u001b[38;5;241m*\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(t_obs)\u001b[38;5;241m/\u001b[39msig_f)\n\u001b[1;32m    625\u001b[0m xi_w_ \u001b[38;5;241m=\u001b[39m xi_w\u001b[38;5;241m*\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(t_obs)\u001b[38;5;241m/\u001b[39msig_f)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iepoch in range(0, n_epoch):\n",
    "    # training ----------------\n",
    "    thismodel = f'seed{seed}_model{modelname}_ep{iepoch}'\n",
    "\n",
    "    env=Task(dt=1)\n",
    "    env.debug=0\n",
    "    # train\n",
    "    if iepoch == 0:\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, device='cpu',clip_range=0.1, ent_coef=0.01)\n",
    "\n",
    "    else:\n",
    "        previous_model = f'seed{seed}_model{modelname}_ep{iepoch-1}'\n",
    "        model = PPO.load(f'ycstore/{previous_model}', env=env)\n",
    "    model.learn(total_timesteps=epoch_size,)\n",
    "    model.save(f'ycstore/{thismodel}')\n",
    "    print(f'epoch{iepoch}, yctore/{thismodel} saved')\n",
    "\n",
    "\n",
    "    # eval collection --------------------\n",
    "    results=[]\n",
    "    thismodel = f'seed{seed}_model{modelname}_ep{iepoch}'\n",
    "    model = PPO.load(f'ycstore/{thismodel}')\n",
    "\n",
    "    def eval_wrapper(a):\n",
    "        episode = run_one_episode(env, model, deterministic=True)\n",
    "        return episode\n",
    "\n",
    "    with multiprocess.Pool(processes=8) as pool:\n",
    "        all_episode_data = pool.map(eval_wrapper, range(num_eval_episode))\n",
    "    \n",
    "    for episode in all_episode_data:\n",
    "        # process\n",
    "        episode['sum_reward']=sum(episode['rewards'])\n",
    "        results.append(episode)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f'avg reward per eval trials: {sum(np.concatenate(df.rewards.to_numpy()))/num_eval_episode:.2f}')\n",
    "    # eval_plot(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.7860), tensor([0.7515]), tensor([0.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belief=env.reset()\n",
    "f, v, w, sig_ff, sig_vv, sig_fv=belief\n",
    "f, v, w, sig_ff, sig_vv, sig_fv\n",
    "f,v,w=env.x\n",
    "f,v,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.012117770500481129, -0.016134459525346756, -0.021752431988716125, -0.02747291326522827, -0.032454680651426315, -0.03550109639763832, -0.03493702411651611, -0.02975146844983101, -0.019988559186458588, -0.006651879288256168, 0.00864340364933014, 0.024282068014144897, 0.039094362407922745, 0.05243770405650139, 0.06407687813043594, 0.0740325003862381, 0.08245997130870819, 0.08956790715456009, 0.09556984156370163, 0.10065941512584686, 0.1050000712275505, 0.10872343182563782, 0.1119321957230568, 0.11470527201890945]\n",
      "num_steps,24\n",
      "actions, (24,)\n",
      "rewards, (24,)\n",
      "states, (25, 3)\n",
      "observations, (24, 1)\n",
      "beliefs, (25, 3)\n",
      "obs_phase_f, torch.Size([10])\n",
      "obs_phase_w, torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# check the spaces of variables.\n",
    "# note that (action, obs) are shorter than states by 1. they are 'transitions'\n",
    "env.timeout=22\n",
    "ep=run_one_episode(env, model, deterministic=True)\n",
    "for k,v in ep.items():\n",
    "    try:\n",
    "        print(f'{k}, {v.shape}')\n",
    "    except:\n",
    "        print(f'{k},{v}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
